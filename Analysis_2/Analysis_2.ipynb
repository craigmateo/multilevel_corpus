{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2\n",
    "## Verbal Corpus of the Dakota Access Pipeline\n",
    "This is the second of 3 analyses of corpus linguistic data related to ecological themes. This corpus consists of articles and webpages related to the Dakota Access Pipeline (DAP). The data was collected manually using a search engine. Results were saves as separate txt files. \n",
    "\n",
    "Link to raw data (on GitHub):\n",
    "\n",
    "[Analysis 2](https://github.com/craigmateo/multilevel_corpus/tree/master/Analysis_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "The code below reads the all the *txt* files and does preprocessing on the text. The preprocessing consists of:\n",
    "\n",
    "1. **Noise removal** (removal of punctuation, special characters, digits)\n",
    "2. **Normalization** (stemming, lemmatization, removal of stopwords) \n",
    "\n",
    "Exceprts from the corpus are then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpt:\n",
      "\n",
      "ric preservation asked army corp engineer conduct formal environmental impact assessment issue envir\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "total=0\n",
    "\n",
    "txt_files = glob.glob(\"C:\\\\Users\\\\Craig\\\\Documents\\\\GitHub\\\\multilevel_corpus\\\\Analysis_2\\\\corpus\\\\*.txt\")\n",
    "raw_lines = []\n",
    "\n",
    "\n",
    "#txt_files = glob.glob(\"*.txt\")\n",
    "for filename in txt_files:\n",
    "\twith open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\tx = f.readlines()\n",
    "\t\tfor line in x:\n",
    "\t\t\traw_lines.append(line)\n",
    "\n",
    "# Libraries for text preprocessing\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#nltk.download('wordnet') \n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "##Creating a list of stop words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "corpus_PRE = []\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, len(raw_lines)):\n",
    "    \n",
    "    #Remove punctuation\n",
    "    #text = re.sub('[^a-zA-Z]', ' ', raw_lines[i])\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", raw_lines[i])\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    corpus_PRE.append(text)\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    ##Stemming\n",
    "    ps=PorterStemmer()\n",
    "    \n",
    "    #Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)\n",
    "\n",
    "text = \" \".join(corpus)\n",
    "\n",
    "print(\"Excerpt:\" + \"\\n\\n\" + text[1100:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count\n",
    "One count is taken with only noise removal and another with both noise removal and normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t****** Word Count ******\n",
      "\n",
      "Noise Removal:\n",
      "\n",
      "291,856\n",
      "\n",
      "Noise Removal & Normalization:\n",
      "\n",
      "174,974\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\t\\t\\t\" + '****** Word Count ******')  \n",
    "\n",
    "textPre = \" \".join(corpus_PRE)\n",
    "\n",
    "num_words_PRE = format(len(textPre.split()),\",\")\n",
    "\n",
    "num_words = format(len(text.split()),\",\")\n",
    "\n",
    "print(\"\\n\" + 'Noise Removal:' + \"\\n\")  \n",
    "\n",
    "print(str(num_words_PRE))\n",
    "\n",
    "print(\"\\n\" + 'Noise Removal & Normalization:' + \"\\n\")  \n",
    "print(str(num_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quotations\n",
    "Quotations are extracted from the corpus using regular expression matching. 500 characters before and after each quotation are also extracted so, for each quote, the context as well as the speaker could be identified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Quotations:\n",
      "\n",
      "1101\n",
      "\n",
      "Sample of Quotation (with context below):\n",
      "\n",
      "\"reshaping the national conversation for any environmental project that would cross the Native American land.\"\n",
      "\n",
      "ny in the Standing Rock tribe considered the pipeline and its intended crossing of the Missouri River to constitute a threat to the region's clean water and to ancient burial grounds. In April 2016, Standing Rock Sioux elder LaDonna Brave Bull Allard established a camp as a center for cultural preservation and spiritual resistance to the pipeline; over the summer the camp grew to thousands of people.  The protests drew considerable  national and international attention and have been said to be \"reshaping the national conversation for any environmental project that would cross the Native American land.\"[5] The U.S. Army Corps of Engineers had conducted a limited review of the route and found no sign\n"
     ]
    }
   ],
   "source": [
    "quotesList = []\n",
    "quoteContext = []\n",
    "total=0\n",
    "\n",
    "raw_text = []\n",
    "for line in raw_lines:\n",
    "    raw_text.append(line)\n",
    "alltext = \" \".join(raw_text)\n",
    "quotes = re.findall(r'\"(.*?)\"', alltext)\n",
    "for i in quotes:\n",
    "    ind = alltext.index(i)\n",
    "    start=ind-500\n",
    "    end=(ind+len(i))+100\n",
    "    context=alltext[start:end]\n",
    "    context = context.replace(\"\\n\",\"\")\n",
    "    quotesList.append(i)\n",
    "    quoteContext.append([i,context])\n",
    "    total=total+1\n",
    "\n",
    "print(\"\\n\" + 'Total Quotations:' + \"\\n\") \n",
    "print(total)\n",
    "\n",
    "print(\"\\n\" + 'Sample of Quotation (with context below):' + \"\\n\") \n",
    "\n",
    "\n",
    "print('\"' + quotesList[0] + '\"' +\"\\n\")\n",
    "print(quoteContext[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Cleaning\n",
    "The initial list of 1101 quotes was manually cleaned by removing noise and lines that were obviously not spoken quotations. Duplicates were also removed. The result was a list of 660 quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number: 988\n",
      "Number removed: 255\n",
      "Final list (deduped): 660\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "colnames = ['QUOTE', 'REMOVE']\n",
    "data = pandas.read_csv(\"C:\\\\Users\\\\Craig\\\\Documents\\\\GitHub\\\\multilevel_corpus\\\\Analysis_2\\\\remove.csv\", names=colnames)\n",
    "quotes = data.QUOTE.tolist()\n",
    "remove = data.REMOVE.tolist()\n",
    "\n",
    "cleanQuotes = []\n",
    "totalRemoved = 0\n",
    "\n",
    "for q in quotes:\n",
    "    ind = quotes.index(q)\n",
    "    if remove[ind]!=\"X\":\n",
    "        if q not in cleanQuotes:\n",
    "            cleanQuotes.append(q)\n",
    "    else:\n",
    "        totalRemoved=totalRemoved+1\n",
    "\n",
    "print(\"Original number: \" + str(len(quotes)))\n",
    "print(\"Number removed: \" + str(totalRemoved))\n",
    "print(\"Final list (deduped): \" +str(len(cleanQuotes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "The 660 quotes were then further reduced manually and qualitatively. Similar quotes (i.e. similar themes/speakers) were removed. Also very short or one-word quotes were generally removed. The 100 or so remaining quotes were then separated into one of three groups:\n",
    "\n",
    "* Group A: proponents who either actively voices support for the pipeline (e.g., company representatives) or took a legal or institutional stand against the pipeline protesters (e.g., law enforcement)\n",
    "* Group B: protesting opponents of the pipeline, most notably the affected Indigenous peoples, but also others who came to Standing Rock, North Dakota to voice opposition\n",
    "* Group C: supporters and allies of protesters, such as NGOs and politicians who spoke out against the pipeline/in support of protesters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total quotes: 92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quote</td>\n",
       "      <td>Speaker</td>\n",
       "      <td>Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Protesters' escalated unlawful behavior this w...</td>\n",
       "      <td>Morton County Sheriff's Department</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...damage caused after protesters set numerous...</td>\n",
       "      <td>Morton County Sheriff's Department</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The police said the protesters had been] very...</td>\n",
       "      <td>Morton County Sheriff's Department</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...multiple archaeological studies conducted w...</td>\n",
       "      <td>Kelcy Warren, CEO of Energy Transfer Partners</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Quote  \\\n",
       "0                                              Quote   \n",
       "1  Protesters' escalated unlawful behavior this w...   \n",
       "2  ...damage caused after protesters set numerous...   \n",
       "3  [The police said the protesters had been] very...   \n",
       "4  ...multiple archaeological studies conducted w...   \n",
       "\n",
       "                                          Speaker  Group  \n",
       "0                                         Speaker  Group  \n",
       "1              Morton County Sheriff's Department      A  \n",
       "2              Morton County Sheriff's Department      A  \n",
       "3              Morton County Sheriff's Department      A  \n",
       "4   Kelcy Warren, CEO of Energy Transfer Partners      A  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames = ['Quote', 'Speaker','Group']\n",
    "\n",
    "data = pandas.read_csv(\"C:\\\\Users\\\\Craig\\\\Documents\\\\GitHub\\\\multilevel_corpus\\\\Analysis_2\\\\grouped.csv\", names=colnames)\n",
    "\n",
    "quotes = data.Quote.tolist()\n",
    "speakers = data.Speaker.tolist()\n",
    "groups = data.Group.tolist()\n",
    "\n",
    "print(\"total quotes: \" +str(len(quotes)-1))\n",
    "\n",
    "data.head() \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Group A \n",
      "\n",
      "          Word  Freq\n",
      "0     pipeline     5\n",
      "1    protester     4\n",
      "2       energy     4\n",
      "3          law     4\n",
      "4        state     3\n",
      "5     transfer     3\n",
      "6      partner     3\n",
      "7      federal     3\n",
      "8       people     3\n",
      "9        think     3\n",
      "10      others     3\n",
      "11     company     3\n",
      "12    behavior     2\n",
      "13      safety     2\n",
      "14      caused     2\n",
      "15      police     2\n",
      "16        said     2\n",
      "17  aggressive     2\n",
      "18       would     2\n",
      "19      cannot     2\n",
      "\n",
      " Group B \n",
      "\n",
      "          Word  Freq\n",
      "0       people     9\n",
      "1       nation     6\n",
      "2         iowa     6\n",
      "3   indigenous     5\n",
      "4       dakota     5\n",
      "5   government     5\n",
      "6        right     5\n",
      "7        water     4\n",
      "8      project     4\n",
      "9        going     4\n",
      "10        land     4\n",
      "11      trying     3\n",
      "12       would     3\n",
      "13         say     3\n",
      "14    industry     3\n",
      "15         get     3\n",
      "16         far     3\n",
      "17        pipe     3\n",
      "18       force     3\n",
      "19         use     3\n",
      "\n",
      " Group C \n",
      "\n",
      "          Word  Freq\n",
      "0       people    13\n",
      "1        going    10\n",
      "2         camp     8\n",
      "3        water     7\n",
      "4      protect     7\n",
      "5     pipeline     6\n",
      "6       prayer     6\n",
      "7        right     6\n",
      "8    something     6\n",
      "9        fight     5\n",
      "10        life     5\n",
      "11  indigenous     5\n",
      "12      future     5\n",
      "13      sacred     4\n",
      "14       think     4\n",
      "15       human     4\n",
      "16        need     4\n",
      "17      mother     4\n",
      "18       earth     4\n",
      "19        like     3\n"
     ]
    }
   ],
   "source": [
    "groupA = []\n",
    "groupB = []\n",
    "groupC = []\n",
    "\n",
    "groupAt = []\n",
    "groupBt = []\n",
    "groupCt = []\n",
    "\n",
    "for i in range(0,len(quotes)):\n",
    "    if groups[i]==\"A\":\n",
    "        groupA.append(quotes[i])\n",
    "    if groups[i]==\"B\":\n",
    "        groupB.append(quotes[i])\n",
    "    if groups[i]==\"C\":\n",
    "        groupC.append(quotes[i])\n",
    "\n",
    "def process(group,target):\n",
    "    \n",
    "    post = []\n",
    "    postT = []\n",
    "    \n",
    "    for i in range(0, len(group)):\n",
    "\n",
    "        #Remove punctuation\n",
    "        text = re.sub('[^a-zA-Z]', ' ', group[i])\n",
    "\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "        postT.append(text)\n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "\n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        post.append(text)\n",
    "    group.append(post)\n",
    "    target.append(postT)\n",
    "\n",
    "process(groupA,groupAt)\n",
    "process(groupB,groupBt)\n",
    "process(groupC,groupCt)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(groupA[-1])\n",
    "\n",
    "list(cv.vocabulary_.keys())[:10]\n",
    " \n",
    "#Most frequently occuring words\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "\n",
    "top_wordsA = get_top_n_words(groupA[-1], n=20)\n",
    "top_wordsB = get_top_n_words(groupB[-1], n=20)\n",
    "top_wordsC = get_top_n_words(groupC[-1], n=20)\n",
    "top_dfA = pandas.DataFrame(top_wordsA)\n",
    "top_dfA.columns=[\"Word\", \"Freq\"]\n",
    "top_dfB = pandas.DataFrame(top_wordsB)\n",
    "top_dfB.columns=[\"Word\", \"Freq\"]\n",
    "top_dfC = pandas.DataFrame(top_wordsC)\n",
    "top_dfC.columns=[\"Word\", \"Freq\"]\n",
    "\n",
    "print(\"\\n Group A \\n\")\n",
    "\n",
    "print(top_dfA)\n",
    "\n",
    "print(\"\\n Group B \\n\")\n",
    "\n",
    "print(top_dfB)\n",
    "\n",
    "print(\"\\n Group C \\n\")\n",
    "print(top_dfC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'development': 13, 'and': 9, 'independence': 7, 'infrastructure': 5, 'projects': 5, 'products': 4, 'model': 4, 'that': 3, 'secretary': 3, 'board': 3, 'partners': 3, 'supply': 3, 'hide': 2, 'regulatory': 2, 'consumption': 2, 'it': 2, 'which': 2, 'in': 2, 'project': 2, 'energy': 2, 'sources': 2, 'on': 2, 'economy': 2, 'an': 2, 'security': 1, 'she': 1, 'trailers': 1, 'the': 1, 'sector': 1, 'firms': 1, 'boom': 1, 'trump': 1, 'foundation': 1, 'colonial': 1, 'information': 1, 'companies': 1, 'away': 1, 'to': 1, 'received': 1, 'even': 1, 'production': 1, 'generation': 1, 'resource': 1, 'indigenous': 1, 'see': 1, 'access': 1, 'those': 1, 'economics': 1, 'giant': 1, 'presidency': 1, 'is': 1, 'protesters': 1, 'industry': 1, 'issues': 1, 'corridor': 1, 'systems': 1, 'commission': 1, 'businesses': 1, 'resources': 1, 'financing': 1, 'additional': 1, 'industries': 1, 'will': 1, 'td': 1, 'campaigner': 1, 'he': 1, 'benefits': 1, 'but': 1, 'of': 1, 'independent': 1, 'here': 1, 'renaissance': 1, 'company': 1, 'strategy': 1})\n"
     ]
    }
   ],
   "source": [
    "concord = []\n",
    "text_list = textPre.split()\n",
    "\n",
    "def getConcord(targTerm, c1):\n",
    "    for i in range(0,len(text_list)):\n",
    "        if targTerm in text_list[i]:\n",
    "            snippet = \" \".join(text_list[i-25:i+25])\n",
    "            loc = snippet.index(targTerm)\n",
    "            line = snippet[loc-35:loc+42]\n",
    "            if line not in c1:\n",
    "                c1.append(line)\n",
    "\n",
    "getConcord(\"energy\", concord)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "lst = []\n",
    "\n",
    "for i in concord:\n",
    "    if \"transfer\" not in i:\n",
    "        sp = i.split()\n",
    "        #print(sp)\n",
    "        for j in sp:\n",
    "            if j==\"energy\":\n",
    "                ind = sp.index(j)\n",
    "                lst.append(sp[ind+1])\n",
    "                \n",
    "result = Counter(lst)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group A\n",
      "All: 0.05\n",
      "Personal plural: 0.02\n",
      "\n",
      "Group B\n",
      "All: 0.11\n",
      "Personal plural: 0.06\n",
      "\n",
      "Group C\n",
      "All: 0.12\n",
      "Personal plural: 0.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt_pronouns = open(\"C:\\\\Users\\\\Craig\\\\Documents\\\\GitHub\\\\multilevel_corpus\\\\Analysis_2\\\\pronouns.txt\")\n",
    "pronouns = txt_pronouns.readlines()\n",
    "pronouns = [line[:-1] for line in pronouns]\n",
    "#print(pronouns)\n",
    "\n",
    "personPl = [\"we\",\"us\",\"ours\",\"our\",\"ourselves\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "def proMatch(group):\n",
    "    countPersonPl=0\n",
    "    countTot=0\n",
    "    group = group[0]\n",
    "    group = list(itertools.chain.from_iterable(group))\n",
    "    #print(group)\n",
    "    length = len(group)\n",
    "    for i in personPl:\n",
    "        for j in group:\n",
    "            if i==j:\n",
    "                ind = group.index(j)\n",
    "                countPersonPl=countPersonPl+1\n",
    "                \n",
    "    for i in pronouns:\n",
    "        for j in group:\n",
    "           if i==j:\n",
    "                countTot=countTot+1 \n",
    "    print(\"All: \" + str(round(countTot/length,2)))\n",
    "    print(\"Personal plural: \" + str(round(countPersonPl/length,2)) + \"\\n\")\n",
    "    \n",
    "\n",
    "print(\"Group A\")\n",
    "proMatch(groupAt)\n",
    "\n",
    "print(\"Group B\")\n",
    "proMatch(groupBt)\n",
    "\n",
    "print(\"Group C\")\n",
    "proMatch(groupCt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuote(targTerm, group):\n",
    "    c1 = []\n",
    "    group = group[0]\n",
    "    group = list(itertools.chain.from_iterable(group))\n",
    "    #print(group)\n",
    "    for i in range(0,len(group)):\n",
    "        if targTerm in group[i]:\n",
    "            snippet = \" \".join(group)\n",
    "            #print(snippet)\n",
    "\n",
    "    \n",
    "getQuote(\"protestor\", groupAt)\n",
    "getQuote(\"protester\", groupAt)\n",
    "#getConcord(\"economy\", groupCt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protesters' escalated unlawful behavior this weekend by setting up illegal roadblocks, trespassing onto private property…this is a public safety issue.     Morton County Sheriff's Department\n",
      "\n",
      "\n",
      "...damage caused after protesters set numerous fires.   Morton County Sheriff's Department\n",
      "\n",
      "\n",
      "[The police said the protesters had been] very aggressive   Morton County Sheriff's Department\n",
      "\n",
      "\n",
      "...multiple archaeological studies conducted with state historic preservation offices found no sacred items along the route   Kelcy Warren, CEO of Energy Transfer Partners\n",
      "\n",
      "\n",
      "...political interference…further delay in the consideration of this case would add millions of dollars more each month in costs which cannot be recovered.   Energy Transfer Partners\n",
      "\n",
      "\n",
      "...will only prolong the disruption in the region caused by protests and make life difficult for everyone who lives and works in the area.   North Dakota Senator John Hoeven\n",
      "\n",
      "\n",
      "[Energy Transfer Partners alleges Greenpeace and other] eco-terrorist groups [tried to block its pipeline with] campaigns of misinformation.   Energy Transfer Partners\n",
      "\n",
      "\n",
      "The protesters' sprawling encampments, with virtually no sanitation facilities, and their contamination of the land and water during their `occupation,' were all in violation of federal law.   Attorney General Wayne Stenehjem\n",
      "\n",
      "\n",
      "a gift to the people of North Dakota (referring to a donation from Energy Transfer partners)   Attorney General Wayne Stenehjem \n",
      "\n",
      "\n",
      "We think this is a great step forward for energy security in America.   Ron Ness, the council's president\n",
      "\n",
      "\n",
      "We are very pleased to bring this important infrastructure project that benefits all Americans and our national economy into service on June 1.   Energy Transfer spokeswoman Lisa Dillinger\n",
      "\n",
      "\n",
      "There were some that would have liked to have it zigzag through their farms, mainly because of what they got paid.   MAIN Coalition Chairman Ed Wiederstein\n",
      "\n",
      "\n",
      "We think that this is a better and safer way to do it…. We have thousands of miles of pipeline through the state of Iowa…the newer approach that was used in this pipeline I think will be a lot safer.   Iowa Gov. Terry Branstad\n",
      "\n",
      "\n",
      "While we can expect to see the continued spread of the anti-DAPL diaspora … aggressive intelligence preparation of the battlefield and active coordination between intelligence and security elements are now a proven method of defeating pipeline insurgencies,   TigerSwan documents [private security firm]\n",
      "\n",
      "\n",
      "Unfortunately, a lot of times these things can be overwhelmed from outside groups.   Sen. Scott Martin\n",
      "\n",
      "\n",
      "...developed response and action plans, and will include several monitoring systems, shut-off valves, and other safety features to minimize the risk of spills…..   Energy Transfer Partners\n",
      "\n",
      "\n",
      "a large component [of protestors] is very violent, very confrontational…we hopefully will see federal agents helping police…. When you have that many people engaged in that kind of behavior, inciting others to break the law, cheering others on as they do break the law, refusing to leave when they're asked to leave, that's not a protest.   Cass County Sheriff Paul Laney\n",
      "\n",
      "\n",
      "Energy Transfer Partners said the project meets “all applicable federal, state and local environmental laws, regulations and standards,” according to a company fact sheet. “We continually seek ways to enhance our operations in the areas of environmental and resource protection and conservation,” the company says.   Energy Transfer Partners\n",
      "\n",
      "\n",
      "There is an element there of people protesting who are frightening. It's time for them to go home.   North Dakota Attorney General Wayne Stenehjem \n",
      "\n",
      "\n",
      "We cannot let the politics of extreme activists, or the narcissistic antics of celebrities, harm what should be our most important goal, which is comity between tribal and non-tribal communities and a unified, neighborly spirit as North Dakotans.   Rob Port, blogger\n",
      "\n",
      "\n",
      "Today's unfortunate decision sends a very chilling signal to others who want to build infrastructure in this country.   Rep. Kevin Cramer\n",
      "\n",
      "\n",
      "This action is motivated purely by politics at the expense of a company that has done nothing but play by the rules it was given,   Energy Transfer Partners CEO, Kelcy Warren\n",
      "\n",
      "\n",
      "We're not in a position where we can agree to any kind of stopping of the pipeline.   David Debold, a lawyer for Dakota Access\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(groups)):\n",
    "    if groups[i]==\"A\":\n",
    "        print(quotes[i], speakers[i])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'peaceful': 17, 'american': 13, 'other': 8, 'indigenous': 8, 'august': 8, 'unruly': 6, 'november': 3, 'nodapl': 3, 'alive': 3, 'native': 3, 'many': 2, 'unarmed': 2, 'allied': 2, 'familiar': 1, 'sunday': 1, 'hundred': 1, 'dapl': 1, 'tribal': 1, 'april': 1, 'future': 1, 'saturday': 1, 'early': 1, 'text': 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "c1 = []\n",
    "\n",
    "def getAdj(targTerm, group):\n",
    "    \n",
    "    for i in range(0,len(group)):\n",
    "        if targTerm in group[i]:\n",
    "            snippet = word_tokenize(group[i])\n",
    "            pos = nltk.pos_tag(snippet)\n",
    "            for i in pos:\n",
    "                \n",
    "                if targTerm in i[0]:\n",
    "                    ind = pos.index(i)\n",
    "                    if pos[ind-1][1]=='JJ':\n",
    "                        c1.append(pos[ind-1][0])   \n",
    "\n",
    "           \n",
    "getAdj(\"protester\", corpus_PRE)\n",
    "getAdj(\"protestor\", corpus_PRE)\n",
    "resultc1 = Counter(c1)\n",
    "print(resultc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
